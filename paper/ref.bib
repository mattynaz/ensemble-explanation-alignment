@inproceedings{smoothgrad2017,
    title = "SmoothGrad: removing noise by adding noise",
    author = "Smilkov, Daniel and 
    Thorat, Nikhil and 
    Kim, Been and
    Viegas, Fernanda and 
    Wattenberg, Martin M",
    booktitle = "ICML Workshop on Visualization for Deep Learning",
    year = "2017",
    address = "Sydney, Australia",
    url = "https://arxiv.org/abs/1706.03825",
    doi = "0.48550/arXiv.1706.03825",
}

@inproceedings{brunet2022,
 author = {Brunet, Marc-Etienne and Anderson, Ashton and Zemel, Richard},
 booktitle = {Advances in Neural Information Processing Systems},
 publisher = {Curran Associates, Inc.},
 title = {Implications of Model Indeterminacy for Explanations
of Automated Decisions},
 url = {https://openreview.net/pdf?id=LzbrVf-l0Xq},
 year = {2022}
}

@misc{krishna2022,
  doi = {10.48550/ARXIV.2202.01602},
  
  url = {https://arxiv.org/abs/2202.01602},
  
  author = {Krishna, Satyapriya and Han, Tessa and Gu, Alex and Pombra, Javin and Jabbari, Shahin and Wu, Steven and Lakkaraju, Himabindu},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {The Disagreement Problem in Explainable Machine Learning: A Practitioner's Perspective},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@inproceedings{garipov2018,
  title={Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs},
  author={Garipov, Timur and Izmailov, Pavel and Podoprikhin, Dmitrii and Vetrov, Dmitry P and Wilson, Andrew Gordon},
  booktitle={Advances in Neural Information Processing Systems},
  year={2018}
}

@inproceedings{fort2019,
  author       = {Stanislav Fort and
                  Stanislaw Jastrzebski},
  title        = {Large Scale Structure of Neural Network Loss Landscapes},
  journal      = {CoRR},
  volume       = {abs/1906.04724},
  year         = {2019},
  url          = {http://arxiv.org/abs/1906.04724},
  eprinttype    = {arXiv},
  eprint       = {1906.04724},
  booktitle={Advances in Neural Information Processing Systems},
}

@inproceedings{black2021selective,
title={Selective Ensembles for Consistent Predictions},
author={Emily Black and Klas Leino and Matt Fredrikson},
year={2021},
booktitle={International Conference on Learning Representations},
eprint={2111.08230},
archivePrefix={arXiv},
primaryClass={cs.LG}
}

@inproceedings{wu2020,
  author       = {Dongxian Wu and
                  Yisen Wang and
                  Shutao Xia},
  title        = {Adversarial Weight Perturbation Helps Robust Generalization},
  journal      = {CoRR},
  volume       = {abs/2004.05884},
  year         = {2020},
  url          = {https://arxiv.org/abs/2004.05884},
  eprinttype    = {arXiv},
  eprint       = {2004.05884},
  timestamp    = {Thu, 14 Oct 2021 09:15:10 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2004-05884.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  booktitle = {Advances in Neural Information Processing Systems}
}

@misc{uci2017,
author = "Dua, Dheeru and Graff, Casey",
year = "2017",
title = "{UCI} Machine Learning Repository",
url = "http://archive.ics.uci.edu/ml",
institution = "University of California, Irvine, School of Information and Computer Sciences",
note = {Accessed on 2023-05-16}
}

@misc{heloc,
    author="FICO",
    year = "2018",
    title="Explainable Machine Learning Challenge",
    url = "https://community.fico.com/s/explainable-machine-learning-challenge?tabset-158d9=d157e",
    note = {Accessed on 2023-05-16}
}

@misc{gmsc,
    author="Bryce Freshcorn",
    year = "2022",
    title="Give me some credit :: 2011 competition data | kaggle",
    url = "https://www.kaggle.com/datasets/brycecf/give-me-some-credit-dataset",
    note = {Accessed on 2023-05-16}
}

@inproceedings{upadhyay2021,
 author = {Upadhyay, Sohini and Joshi, Shalmali and Lakkaraju, Himabindu},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {16926--16937},
 publisher = {Curran Associates, Inc.},
 title = {Towards Robust and Reliable Algorithmic Recourse},
 url = {https://proceedings.neurips.cc/paper/2021/file/8ccfb1140664a5fa63177fb6e07352f0-Paper.pdf},
 volume = {34},
 year = {2021}
}

@misc{rawal2020,
  doi = {10.48550/ARXIV.2012.11788},
  
  url = {https://arxiv.org/abs/2012.11788},
  
  author = {Rawal, Kaivalya and Kamar, Ece and Lakkaraju, Himabindu},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Algorithmic Recourse in the Wild: Understanding the Impact of Data and Model Shifts},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@misc{xin2022,
  doi = {10.48550/ARXIV.2209.08040},
  
  url = {https://arxiv.org/abs/2209.08040},
  
  author = {Xin, Rui and Zhong, Chudi and Chen, Zhi and Takagi, Takuya and Seltzer, Margo and Rudin, Cynthia},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Exploring the Whole Rashomon Set of Sparse Decision Trees},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{li2021,
  doi = {10.48550/ARXIV.2109.00707},
  url = {https://arxiv.org/abs/2109.00707},
  author = {Li, Xuhong and Xiong, Haoyi and Huang, Siyu and Ji, Shilei and Dou, Dejing},
  
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Cross-Model Consensus of Explanations and Beyond for Image Classification Models: An Empirical Study},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{damour2022,
  author  = {Alexander D'Amour and Katherine Heller and Dan Moldovan and Ben Adlam and Babak Alipanahi and Alex Beutel and Christina Chen and Jonathan Deaton and Jacob Eisenstein and Matthew D. Hoffman and Farhad Hormozdiari and Neil Houlsby and Shaobo Hou and Ghassen Jerfel and Alan Karthikesalingam and Mario Lucic and Yian Ma and Cory McLean and Diana Mincu and Akinori Mitani and Andrea Montanari and Zachary Nado and Vivek Natarajan and Christopher Nielson and Thomas F. Osborne and Rajiv Raman and Kim Ramasamy and Rory Sayres and Jessica Schrouff and Martin Seneviratne and Shannon Sequeira and Harini Suresh and Victor Veitch and Max Vladymyrov and Xuezhi Wang and Kellie Webster and Steve Yadlowsky and Taedong Yun and Xiaohua Zhai and D. Sculley},
  title   = {Underspecification Presents Challenges for Credibility in Modern Machine Learning},
  journal = {Journal of Machine Learning Research},
  year    = {2022},
  volume  = {23},
  number  = {226},
  pages   = {1--61},
  url     = {http://jmlr.org/papers/v23/20-1335.html}
}

@misc{yang2021,
  doi = {10.48550/ARXIV.2107.10873},
  
  url = {https://arxiv.org/abs/2107.10873},
  
  author = {Yang, Zhuolin and Li, Linyi and Xu, Xiaojun and Kailkhura, Bhavya and Xie, Tao and Li, Bo},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Cryptography and Security (cs.CR), Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {On the Certified Robustness for Ensemble Models and Beyond},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {Creative Commons Attribution Non Commercial No Derivatives 4.0 International}
}

@inproceedings{benton2021,
  title={Loss Surface Simplexes for Mode Connecting Volumes and Fast Ensembling},
  author={Benton, Gregory W and Maddox, Wesley J and Lotfi, Sanae and Wilson, Andrew Gordon},
  booktitle={International Conference on Machine Learning},
  year={2021}
}

@inproceedings{zhao2020,
  author       = {Pu Zhao and
                  Pin{-}Yu Chen and
                  Payel Das and
                  Karthikeyan Natesan Ramamurthy and
                  Xue Lin},
  title        = {Bridging Mode Connectivity in Loss Landscapes and Adversarial Robustness},
  booktitle={International Conference on Learning Representations},
  year         = {2020},
  url          = {https://arxiv.org/abs/2005.00060},
}

@inproceedings{draxler2019,
      title={Essentially No Barriers in Neural Network Energy Landscape}, 
      author={Felix Draxler and Kambis Veschgini and Manfred Salmhofer and Fred A. Hamprecht},
      year={2019},
      eprint={1803.00885},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      booktitle={International Conference on Machine Learning},
}

@inproceedings{izmailov2018,
  author       = {Pavel Izmailov and
                  Dmitrii Podoprikhin and
                  Timur Garipov and
                  Dmitry P. Vetrov and
                  Andrew Gordon Wilson},
  title        = {Averaging Weights Leads to Wider Optima and Better Generalization},
  journal      = {CoRR},
  volume       = {abs/1803.05407},
  year         = {2018},
  url          = {http://arxiv.org/abs/1803.05407},
  eprinttype    = {arXiv},
  eprint       = {1803.05407},
  booktitle    = {Conference on Uncertainty in Artificial Intelligence}
}

@inproceedings{ainsworth2023,
      title={Git Re-Basin: Merging Models modulo Permutation Symmetries}, 
      author={Samuel K. Ainsworth and Jonathan Hayase and Siddhartha Srinivasa},
      year={2023},
      eprint={2209.04836},
      booktitle = {International Conference on Learning Representations},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{gotmare2018,
  author       = {Akhilesh Gotmare and
                  Nitish Shirish Keskar and
                  Caiming Xiong and
                  Richard Socher},
  title        = {Using Mode Connectivity for Loss Landscape Analysis},
  journal      = {CoRR},
  volume       = {abs/1806.06977},
  year         = {2018},
  url          = {http://arxiv.org/abs/1806.06977},
  eprinttype    = {arXiv},
  eprint       = {1806.06977},
  booktitle    = {ICML Workshop on Modern Trends in Nonconvex Optimization for Machine Learning,}
}

@inproceedings{tatro2020,
      title={Optimizing Mode Connectivity via Neuron Alignment}, 
      author={N. Joseph Tatro and Pin-Yu Chen and Payel Das and Igor Melnyk and Prasanna Sattigeri and Rongjie Lai},
      year={2020},
      eprint={2009.02439},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      booktitle = {Advances in Neural Information Processing Systems}
}

@inproceedings{singh2020,
  author       = {Sidak Pal Singh and
                  Martin Jaggi},
  title        = {Model Fusion via Optimal Transport},
  journal      = {CoRR},
  volume       = {abs/1910.05653},
  year         = {2020},
  url          = {http://arxiv.org/abs/1910.05653},
  eprinttype    = {arXiv},
  eprint       = {1910.05653},
  booktitle = {Advances in Neural Information Processing Systems}
}

%%%%%% FIX THESE CITATIONS

@misc{gdpr,
author={GDPR},
year = 2018,
title={General {D}ata {P}rotection {R}egulation ({GDPR})},
url={https://gdpr.eu/tag/gdpr/},
organization = {European Commission},
date = {2018-05-25},
}

@misc{aibillofrights,
    author = {AI-Rights},
    year = {2022},
    title={Blueprint for an {AI} {B}ill of {R}ights},
    organization = {Office of Science and Technology Policy},
    url = {https://www.whitehouse.gov/ostp/ai-bill-of-rights/}
}

%%%%%%%%%%%%%%%%%%%%%%%%%

@inproceedings{lundberg2017,
 author = {Lundberg, Scott M and Lee, Su-In},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {A Unified Approach to Interpreting Model Predictions},
 url = {https://proceedings.neurips.cc/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Paper.pdf},
 volume = {30},
 year = {2017}
}

@inproceedings{ribeiro2016,
author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
title = {"Why Should I Trust You?": Explaining the Predictions of Any Classifier},
year = {2016},
isbn = {9781450342322},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2939672.2939778},
doi = {10.1145/2939672.2939778},
booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1135–1144},
numpages = {10},
keywords = {explaining machine learning, interpretable machine learning, interpretability, black box classifier},
location = {San Francisco, California, USA},
series = {KDD '16},
}

@article{simonyan2013,
  title={Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps},
  author={Karen Simonyan and Andrea Vedaldi and Andrew Zisserman},
  journal={CoRR},
  year={2013},
  volume={abs/1312.6034}
}

@inproceedings{han2022,
title={Which Explanation Should {I} Choose? {A} Function Approximation Perspective to Characterizing Post Hoc Explanations},
author={Tessa Han and Suraj Srinivas and Himabindu Lakkaraju},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=rTvH1_SRyXs}
}

@article{doshivelez2017,
  title={Towards A Rigorous Science of Interpretable Machine Learning},
  author={Finale Doshi-Velez and Been Kim},
  journal={arXiv: Machine Learning},
  year={2017}
}

@inproceedings{koh2017,
author = {Koh, Pang Wei and Liang, Percy},
title = {Understanding Black-Box Predictions via Influence Functions},
year = {2017},
publisher = {JMLR.org},
abstract = {How can we explain the predictions of a black-box model? In this paper, we use influence functions — a classic technique from robust statistics — to trace a model's prediction through the learning algorithm and back to its training data, thereby identifying training points most responsible for a given prediction. To scale up influence functions to modern machine learning settings, we develop a simple, efficient implementation that requires only oracle access to gradients and Hessian-vector products. We show that even on non-convex and non-differentiable models where the theory breaks down, approximations to influence functions can still provide valuable information. On linear models and convolutional neural networks, we demonstrate that influence functions are useful for multiple purposes: understanding model behavior, debugging models, detecting dataset errors, and even creating visually-indistinguishable training-set attacks.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {1885–1894},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{kolen1990,
author = {Kolen, John F. and Pollack, Jordan B.},
title = {Back Propagation is Sensitive to Initial Conditions},
year = {1990},
isbn = {1558601848},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
booktitle = {Proceedings of the 1990 Conference on Advances in Neural Information Processing Systems 3},
pages = {860–867},
numpages = {8},
location = {Denver, Colorado, USA},
series = {NIPS-3}
}

@article {mehrer2020,
	Title = {Individual differences among deep neural network models},
	Author = {Mehrer, Johannes and Spoerer, Courtney J and Kriegeskorte, Nikolaus and Kietzmann, Tim C},
	DOI = {10.1038/s41467-020-19632-w},
	Number = {1},
	Volume = {11},
	Month = {November},
	Year = {2020},
	Journal = {Nature communications},
	ISSN = {2041-1723},
	Pages = {5725},
	Abstract = {Deep neural networks (DNNs) excel at visual recognition tasks and are increasingly used as a modeling framework for neural computations in the primate brain. Just like individual brains, each DNN has a unique connectivity and representational profile. Here, we investigate individual differences among DNN instances that arise from varying only the random initialization of the network weights. Using tools typically employed in systems neuroscience, we show that this minimal change in initial conditions prior to training leads to substantial differences in intermediate and higher-level network representations despite similar network-level classification performance. We locate the origins of the effects in an under-constrained alignment of category exemplars, rather than misaligned category centroids. These results call into question the common practice of using single networks to derive insights into neural information processing and rather suggest that computational neuroscientists working with DNNs may need to base their inferences on groups of multiple network instances.},
	URL = {https://europepmc.org/articles/PMC7665054},
}

@inproceedings{black2021unfairness,
    author = {Black, Emily and Fredrikson, Matt},
    title = {Leave-One-out Unfairness},
    year = {2021},
    isbn = {9781450383097},
    publisher = {Association for Computing Machinery},
    url = {https://doi.org/10.1145/3442188.3445894},
    doi = {10.1145/3442188.3445894},
    booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
    pages = {285–295},
    numpages = {11},
    location = {Virtual Event, Canada},
    series = {FAccT '21}
}


@inproceedings{glorot2010,
  title = 	 {Understanding the difficulty of training deep feedforward neural networks},
  author = 	 {Glorot, Xavier and Bengio, Yoshua},
  booktitle = 	 {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {249--256},
  year = 	 {2010},
  editor = 	 {Teh, Yee Whye and Titterington, Mike},
  volume = 	 {9},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Chia Laguna Resort, Sardinia, Italy},
  month = 	 {13--15 May},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf},
  url = 	 {https://proceedings.mlr.press/v9/glorot10a.html},
  abstract = 	 {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future.  We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1.  Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.}
}

@inproceedings{lakshminarayanan2017,
      title={Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles}, 
      author={Balaji Lakshminarayanan and Alexander Pritzel and Charles Blundell},
      year={2017},
      eprint={1612.01474},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      booktitle = {Advances in Neural Information Processing Systems},
}

@inproceedings{agarwal2022,
title={Open{XAI}: Towards a Transparent Evaluation of Model Explanations},
author={Chirag Agarwal and Satyapriya Krishna and Eshika Saxena and Martin Pawelczyk and Nari Johnson and Isha Puri and Marinka Zitnik and Himabindu Lakkaraju},
booktitle={Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
year={2022},
url={https://openreview.net/forum?id=MU2495w47rz}
}

@inproceedings{allenzhu2023,
      title={Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning}, 
      author={Zeyuan Allen-Zhu and Yuanzhi Li},
      year={2023},
      booktitle={International Conference on Learning Representations},
      eprint={2012.09816},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{hinton2015,
title	= {Distilling the Knowledge in a Neural Network},
author	= {Geoffrey Hinton and Oriol Vinyals and Jeffrey Dean},
year	= {2015},
URL	= {http://arxiv.org/abs/1503.02531},
booktitle	= {NIPS Deep Learning and Representation Learning Workshop}
}

@misc{fort2020,
      title={Deep Ensembles: A Loss Landscape Perspective}, 
      author={Stanislav Fort and Huiyi Hu and Balaji Lakshminarayanan},
      year={2020},
      eprint={1912.02757},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@InProceedings{dietterich2000,
author="Dietterich, Thomas G.",
title="Ensemble Methods in Machine Learning",
booktitle="Multiple Classifier Systems",
year="2000",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="1--15",
abstract="Ensemble methods are learning algorithms that construct a set of classifiers and then classify new data points by taking a (weighted) vote of their predictions. The original ensemble method is Bayesian averaging, but more recent algorithms include error-correcting output coding, Bagging, and boosting. This paper reviews these methods and explains why ensembles can often perform better than any single classifier. Some previous studies comparing ensemble methods are reviewed, and some new experiments are presented to uncover the reasons that Adaboost does not overfit rapidly.",
isbn="978-3-540-45014-6"
}

@article{yeh2009,
title = {The comparisons of data mining techniques for the predictive accuracy of probability of default of credit card clients},
journal = {Expert Systems with Applications},
volume = {36},
number = {2, Part 1},
pages = {2473-2480},
year = {2009},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2007.12.020},
url = {https://www.sciencedirect.com/science/article/pii/S0957417407006719},
author = {Yeh, I-Cheng and Lien, Che-hui},
keywords = {Banking, Neural network, Probability, Data mining},
}

@article{mackay1992,
  title={A practical {B}ayesian framework for backpropagation networks},
  author={MacKay, David JC},
  journal={Neural computation},
  volume={4},
  number={3},
  pages={448--472},
  year={1992},
  publisher={MIT Press}
}

@article{gal2016,
  title={Uncertainty in deep learning},
  author={Gal, Yarin},
  journal={University of Cambridge},
  volume={1},
  number={3},
  year={2016}
}

@article{srivastava2014,
  author  = {Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov},
  title   = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
  journal = {Journal of Machine Learning Research},
  year    = {2014},
  volume  = {15},
  number  = {56},
  pages   = {1929--1958},
  url     = {http://jmlr.org/papers/v15/srivastava14a.html}
}

@inproceedings{paszke2019,
      title={PyTorch: An Imperative Style, High-Performance Deep Learning Library}, 
      author={Adam Paszke and Sam Gross and Francisco Massa and Adam Lerer and James Bradbury and Gregory Chanan and Trevor Killeen and Zeming Lin and Natalia Gimelshein and Luca Antiga and Alban Desmaison and Andreas Köpf and Edward Yang and Zach DeVito and Martin Raison and Alykhan Tejani and Sasank Chilamkurthy and Benoit Steiner and Lu Fang and Junjie Bai and Soumith Chintala},
      year={2019},
      booktitle={Advances in Neural Information Processing Systems},
      eprint={1912.01703},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{shrikumar2019,
      title={Learning Important Features Through Propagating Activation Differences}, 
      author={Avanti Shrikumar and Peyton Greenside and Anshul Kundaje},
      year={2019},
      eprint={1704.02685},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{liaw2018,
      title={Tune: A Research Platform for Distributed Model Selection and Training}, 
      author={Richard Liaw and Eric Liang and Robert Nishihara and Philipp Moritz and Joseph E. Gonzalez and Ion Stoica},
      booktitle = {ICML Workshop on AutoML},
      year={2018},
      eprint={1807.05118},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{joblib2020,
title = {Joblib: running Python functions as pipeline jobs},
author = {{Joblib Development Team}},
year = {2020},
url = {https://joblib.readthedocs.io/},
}

@book{van1995,
  title={Python reference manual},
  author={Van Rossum, Guido and Drake Jr, Fred L},
  year={1995},
  publisher={Centrum voor Wiskunde en Informatica Amsterdam}
}

@article{breiman2001,
author = {Leo Breiman},
title = {{Statistical Modeling: The Two Cultures (with comments and a rejoinder by the author)}},
volume = {16},
journal = {Statistical Science},
number = {3},
publisher = {Institute of Mathematical Statistics},
pages = {199 -- 231},
year = {2001},
doi = {10.1214/ss/1009213726},
URL = {https://doi.org/10.1214/ss/1009213726}
}

@InProceedings{furlanello2018,
  title = 	 {Born Again Neural Networks},
  author =       {Furlanello, Tommaso and Lipton, Zachary and Tschannen, Michael and Itti, Laurent and Anandkumar, Anima},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {1607--1616},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/furlanello18a/furlanello18a.pdf},
  url = 	 {https://proceedings.mlr.press/v80/furlanello18a.html},
}