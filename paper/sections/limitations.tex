\section{Conclusion and limitations}
\label{sec:limitations}

Here, we tackle the challenge of providing consistent explanations for predictive models in the presence of model indeterminacy from the underspecification set. Motivated by local and mode-connected loss landscape exploration, we develop two novel ensembling methods. On five benchmark financial datasets, our methods markedly increase the consistency of model explanations when using a fixed number of pre-trained models. Moreover, our methods are computationally more affordable than standard ensembling techniques with respect to the number of trained model constituents, while not compromising on test accuracy.

However, our work is limited along some dimensions. For each dataset, we select one set of optimal hyperparameters, though there exist many such sets of these that can often cover a broad range. Exploring multiple underspecification sets together is useful future work. Additionally, the methods we provide traverse the loss landscape along two fundamental axes: locally (with perturbations) and globally (between models). The specific methods used for exploring both of these can be further optimized. Finally, although our methods require no extra training compared to standard ensembling, approaches to reduce the total number of models included in the set should be explored to cut inference costs. A more comprehensive discussion on the limitations and prospective avenues of study, including the exploration of alternate explanation methods, the alignment of constituent models in weight space through permutation symmetries \citep{ainsworth2023, singh2020, tatro2020}, and the effects of distillation \citep{hinton2015} and self-distillation \citep{furlanello2018} are outlined in Appendix~\ref{app:limitations}.

It is important to acknowledge that our work does not explicitly consider the broader impacts and desirable properties of these ensembles in application, such as for fairness or bias. While our work serves as a foundation for exploring the impact of ensembling on explanation consistency, we encourage further research into how our techniques may interact with other aspects of AI safety.
