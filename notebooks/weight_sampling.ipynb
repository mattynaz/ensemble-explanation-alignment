{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run once to allow imports\n",
    "%cd ..\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import datasets\n",
    "from datasets.tabular import TabularModelPerturb\n",
    "from datasets import get_model_class\n",
    "from util import get_weight_norm, get_weight_diff\n",
    "from style import plot_grads, bold\n",
    "from similarity import get_top_k, average_pairwise_score, top_k_sa\n",
    "from similarity import average_ground_truth_score, ground_truth_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_sources = ['rs', 'loo']\n",
    "random_source = random_sources[0]\n",
    "print(bold(\"Source of Randomness:\"), random_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'heloc'  # or 'german'\n",
    "n_models = 1000  # 1000 for german if needed\n",
    "trainset, testset = datasets.load_dataset(name)\n",
    "\n",
    "X_test, y_test = testset.data.numpy(), testset.labels.numpy()\n",
    "n_inputs, n_features = X_test.shape\n",
    "model_args = [n_features, datasets.tabular.layers[name]]\n",
    "model_class = get_model_class(name)\n",
    "optim = 'sgd'\n",
    "epochs = 20\n",
    "lr = 0.1\n",
    "batch_size = 64\n",
    "dropout = 0\n",
    "directory = f'models/{name}/{random_source}/{optim}_epochs{epochs}_lr{lr}_batch{batch_size}_dropout{dropout}'\n",
    "print(bold(\"Directory:\"), directory)\n",
    "\n",
    "def load_model(idx):\n",
    "    model_class = get_model_class(name)\n",
    "    model = model_class(*model_args)\n",
    "    state_dict = torch.load(f'{directory}/model_{idx}.pth')\n",
    "    model.load_state_dict(state_dict)\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Layer Weight Norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(0)\n",
    "first_layer = model.state_dict()['network.0.weight'].numpy()\n",
    "print(first_layer.shape)\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15, 5), dpi=100)\n",
    "ax[0].hist(np.linalg.norm(first_layer, axis=1), bins=30)\n",
    "ax[1].hist(np.linalg.norm(first_layer, axis=0), bins=15)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One model, Gaussian noise (mean = 0, std. dev = sigma) on first layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(0)\n",
    "original_grads = model.compute_gradients(X_test, softmax=False, label=1, return_numpy=True)\n",
    "original_preds = model.predict(X_test, return_numpy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 1000\n",
    "sigma = 0.01\n",
    "pert_model = TabularModelPerturb(model, n_samples, sigma)\n",
    "pert_grads = pert_model.compute_gradients(X_test, mean=False)\n",
    "pert_preds = pert_model.predict(X_test, mean=False)\n",
    "pert_preds.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perturbation Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(pert_preds.mean(axis=0), bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perturbation Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pert_grads.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_grads(pert_grads[:10, 0], nrows=2, ncols=5, k=5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_size = 20\n",
    "sigma = 0.1\n",
    "n_samples = 100\n",
    "model_idx = np.random.choice(n_models, ensemble_size, replace=False)\n",
    "grads = np.array([np.load(f'{directory}/grads_{idx}.npy') for idx in model_idx])\n",
    "preds = np.array([np.load(f'{directory}/preds_{idx}.npy') for idx in model_idx])\n",
    "model_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_grads = np.zeros((n_samples, *grads.shape))\n",
    "noisy_preds = np.zeros((n_samples, *preds.shape))\n",
    "layer_str = 'network.0.weight'\n",
    "for j in tqdm(range(len(model_idx))):\n",
    "    for i in range(n_samples):\n",
    "        model = model_class(*model_args)\n",
    "        state_dict = torch.load(f'{directory}/model_{model_idx[j]}.pth')\n",
    "        # Add noise to layer weights\n",
    "        state_dict[layer_str] += torch.randn(state_dict[layer_str].shape) * sigma\n",
    "        model.load_state_dict(state_dict)\n",
    "\n",
    "        # Compute positive class softmax prediction gradient wrt input (softmax=False for logit output)\n",
    "        noisy_grads[i, j] = model.compute_gradients(X_test, softmax=False, label=1, return_numpy=True)\n",
    "        noisy_preds[i, j] = model.predict(X_test, return_numpy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk, s = get_top_k(5, grads, return_sign=True)\n",
    "sa = average_pairwise_score(tk, s, top_k_sa)\n",
    "tk_smooth, s_smooth = get_top_k(5, noisy_grads.mean(axis=0), return_sign=True)\n",
    "sa_smooth = average_pairwise_score(tk_smooth, s_smooth, top_k_sa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n",
    "ax[0].boxplot([sa, sa_smooth], labels=['20 models', '20 models with noise added to weights'])\n",
    "ax[0].set_title('Effect of adding noise to weights on gradient similarity')\n",
    "ax[0].set_ylabel('Pairwise Similarity')\n",
    "\n",
    "smooth_preds = np.where(noisy_preds.mean(axis=0) > 0.5, 1, 0)\n",
    "smooth_acc = (smooth_preds==y_test).mean(axis=1)\n",
    "orig_acc = (preds==y_test).mean(axis=1)\n",
    "ax[1].boxplot([orig_acc, smooth_acc], labels=['20 models', '20 models with noise added to weights'])\n",
    "ax[1].set_title('Effect of adding noise to weights on accuracy')\n",
    "ax[1].set_ylabel('Accuracy')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_preds = np.where(preds.mean(axis=0) > 0.5, 1, 0)\n",
    "smooth_ensemble_preds = np.where(smooth_preds.mean(axis=0) > 0.5, 1, 0)\n",
    "(ensemble_preds==y_test).mean(), (smooth_ensemble_preds==y_test).mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ablations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average of all 1000 trained models\n",
    "sigmas = np.logspace(-2, -0.3, 10)\n",
    "grads_gt = np.array([np.load(f'{directory}/grads_{idx}.npy') for idx in range(n_models)]).mean(axis=0)\n",
    "grads_gt.shape, sigmas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect of $\\sigma$ (perturb one model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(0)\n",
    "grads = model.compute_gradients(X_test, return_numpy=True)\n",
    "grads.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perturb_layers = ['network.0.weight']\n",
    "n_samples = 100\n",
    "pert_grads = np.zeros((len(sigmas), n_samples, n_inputs, n_features))\n",
    "pert_logits = np.zeros((len(sigmas), n_samples, n_inputs, 2))\n",
    "for i, sigma in enumerate(tqdm(sigmas)):\n",
    "    pert_model = TabularModelPerturb(model, n_samples, sigma, perturb_layers=perturb_layers)\n",
    "    pert_grads[i] = pert_model.compute_gradients(X_test, mean=False)\n",
    "    pert_logits[i] = pert_model.compute_logits(X_test, mean=False)\n",
    "accs = (pert_logits.argmax(3)==y_test).mean(axis=2)\n",
    "accs_mean = (pert_logits.mean(axis=1).argmax(2)==y_test).mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topk_og, signs_og = get_top_k(5, grads, return_sign=True)\n",
    "topk_gt, signs_gt = get_top_k(5, grads_gt, return_sign=True)\n",
    "topk_mean, signs_mean = get_top_k(5, pert_grads.mean(axis=1), return_sign=True)\n",
    "topk, signs = get_top_k(5, pert_grads, return_sign=True)\n",
    "\n",
    "# Similarity\n",
    "sa_gt = np.zeros((len(sigmas), n_inputs))\n",
    "sa_og = np.zeros((len(sigmas), n_inputs))\n",
    "for i in tqdm(range(len(sigmas))):\n",
    "    sa_gt[i] = average_ground_truth_score(topk[i], signs[i], topk_gt, signs_gt, top_k_sa)\n",
    "    sa_og[i] = average_ground_truth_score(topk[i], signs[i], topk_og, signs_og, top_k_sa)\n",
    "sa_mean_gt = ground_truth_score(topk_mean, signs_mean, topk_gt, signs_gt, top_k_sa)\n",
    "sa_mean_og = ground_truth_score(topk_mean, signs_mean, topk_og, signs_og, top_k_sa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=4, figsize=(25, 5), dpi=100)\n",
    "sas = [[sa_og, sa_mean_og], [sa_gt, sa_mean_gt]]\n",
    "titles = ['Similarity to base model gradients',\n",
    "          'Similarity to all model gradients',\n",
    "          'Test Accuracy',\n",
    "          'Noise Norm']\n",
    "labels = ['Individual Perturbation', 'Mean of Perturbations (Perturbed Model)']\n",
    "layer_weights = model.state_dict()[perturb_layers[0]]\n",
    "noise = [torch.randn_like(layer_weights) * sigma for sigma in sigmas]\n",
    "for i in range(4):\n",
    "    if i == 3:\n",
    "        ax[i].plot(sigmas, [n.norm().item() for n in noise], label='Noise')\n",
    "        ax[i].plot(sigmas, [get_weight_norm(model.state_dict())]*len(sigmas), label='Full Model')\n",
    "        ax[i].plot(sigmas, [(model.state_dict()[perturb_layers[0]]).norm()]*len(sigmas), label='First Layer')\n",
    "        ax[i].set_ylabel('$\\ell_2$ Norm')\n",
    "    elif i == 2:\n",
    "        q = np.quantile(accs*100, [0.25, 0.5, 0.75], axis=1)\n",
    "        ax[i].plot(sigmas, q[1], label=labels[0])\n",
    "        ax[i].fill_between(sigmas, q[0], q[2], alpha=0.2)\n",
    "        ax[i].plot(sigmas, accs_mean*100, label=labels[1])\n",
    "        ax[i].set_ylabel('Accuracy (%)')\n",
    "    else:\n",
    "        for j in range(2):\n",
    "            q = np.quantile(sas[i][j], [0.25, 0.5, 0.75], axis=1)\n",
    "            ax[i].plot(sigmas, q[1], label=labels[j])\n",
    "            ax[i].fill_between(sigmas, q[0], q[2], alpha=0.2)\n",
    "            ax[i].set_yticks(np.arange(0, 1.01, 0.2))\n",
    "        ax[i].set_ylabel('Average SA score')\n",
    "    ax[i].set_xlabel('$\\sigma$')\n",
    "    ax[i].set_title(titles[i])\n",
    "    ax[i].legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect of $\\sigma$ (perturb multiple models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trials = 20\n",
    "model_idx = np.random.choice(n_models, n_trials, replace=False)\n",
    "model_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perturb_layers = ['network.0.weight']\n",
    "n_samples = 100\n",
    "pert_grads = np.zeros((len(sigmas), len(model_idx), n_inputs, n_features))\n",
    "pert_logits = np.zeros((len(sigmas), len(model_idx), n_inputs, 2))\n",
    "for j, m_idx in enumerate(tqdm(model_idx)):\n",
    "    model = load_model(m_idx)\n",
    "    for i, sigma in enumerate(sigmas):\n",
    "        pert_model = TabularModelPerturb(model, n_samples, sigma, perturb_layers=perturb_layers)\n",
    "        pert_grads[i,j] = pert_model.compute_gradients(X_test, mean=True)\n",
    "        pert_logits[i,j] = pert_model.compute_logits(X_test, mean=True)\n",
    "accs = (pert_logits.argmax(3)==y_test).mean(axis=2)\n",
    "accs_mean = (pert_logits.mean(axis=1).argmax(2)==y_test).mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topk_og, signs_og = get_top_k(5, grads, return_sign=True)\n",
    "topk_gt, signs_gt = get_top_k(5, grads_gt, return_sign=True)\n",
    "topk_mean, signs_mean = get_top_k(5, pert_grads.mean(axis=1), return_sign=True)\n",
    "topk, signs = get_top_k(5, pert_grads, return_sign=True)\n",
    "\n",
    "# Similarity\n",
    "sa_gt = np.zeros((len(sigmas), n_inputs))\n",
    "sa_og = np.zeros((len(sigmas), n_inputs))\n",
    "for i in tqdm(range(len(sigmas))):\n",
    "    sa_gt[i] = average_ground_truth_score(topk[i], signs[i], topk_gt, signs_gt, top_k_sa)\n",
    "    sa_og[i] = average_ground_truth_score(topk[i], signs[i], topk_og, signs_og, top_k_sa)\n",
    "sa_mean_gt = ground_truth_score(topk_mean, signs_mean, topk_gt, signs_gt, top_k_sa)\n",
    "sa_mean_og = ground_truth_score(topk_mean, signs_mean, topk_og, signs_og, top_k_sa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=4, figsize=(25, 5), dpi=100)\n",
    "sas = [[sa_og, sa_mean_og], [sa_gt, sa_mean_gt]]\n",
    "titles = ['Similarity to base model gradients',\n",
    "          'Similarity to all model gradients',\n",
    "          'Test Accuracy',\n",
    "          'Noise Norm']\n",
    "labels = ['Perturbed Models', 'Ensemble of Perturbed Models']\n",
    "layer_weights = model.state_dict()[perturb_layers[0]]\n",
    "noise = [torch.randn_like(layer_weights) * sigma for sigma in sigmas]\n",
    "for i in range(4):\n",
    "    if i == 3:\n",
    "        ax[i].plot(sigmas, [n.norm().item() for n in noise], label='Noise')\n",
    "        ax[i].plot(sigmas, [get_weight_norm(model.state_dict())]*len(sigmas), label='Full Model')\n",
    "        ax[i].plot(sigmas, [(model.state_dict()[perturb_layers[0]]).norm()]*len(sigmas), label='First Layer')\n",
    "        ax[i].set_ylabel('$\\ell_2$ Norm')\n",
    "    elif i == 2:\n",
    "        q = np.quantile(accs*100, [0.25, 0.5, 0.75], axis=1)\n",
    "        ax[i].plot(sigmas, q[1], label=labels[0])\n",
    "        ax[i].fill_between(sigmas, q[0], q[2], alpha=0.2)\n",
    "        ax[i].plot(sigmas, accs_mean*100, label=labels[1])\n",
    "        ax[i].set_ylabel('Accuracy (%)')\n",
    "    else:\n",
    "        for j in range(2):\n",
    "            q = np.quantile(sas[i][j], [0.25, 0.5, 0.75], axis=1)\n",
    "            ax[i].plot(sigmas, q[1], label=labels[j])\n",
    "            ax[i].fill_between(sigmas, q[0], q[2], alpha=0.2)\n",
    "            ax[i].set_yticks(np.arange(0, 1.01, 0.2))\n",
    "        ax[i].set_ylabel('SA score')\n",
    "    ax[i].set_xlabel('$\\sigma$')\n",
    "    ax[i].set_title(titles[i])\n",
    "    ax[i].legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect of $\\sigma$ and number of perturbations (similarity between ensembles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "5/np.sqrt(64*128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perturb_layers = ['network.0.weight']\n",
    "n_trials = 20\n",
    "ensemble_size = 1\n",
    "n_samples = [10, 20, 50, 100, 200]\n",
    "sigmas = np.logspace(-2, -0.3, 20)\n",
    "sigmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pert_grads = np.zeros((len(sigmas), len(n_samples), n_trials, n_inputs, n_features))\n",
    "pert_logits = np.zeros((len(sigmas), len(n_samples), n_trials, n_inputs, 2))\n",
    "for j in tqdm(range(n_trials)):\n",
    "    model_idx = np.random.choice(n_models, ensemble_size, replace=False)\n",
    "    models = [load_model(i) for i in model_idx]\n",
    "    for i, sigma in enumerate(sigmas):\n",
    "        for model in models:\n",
    "            pert_model = TabularModelPerturb(model, n_samples[-1], sigma, perturb_layers=perturb_layers)\n",
    "            p_grads = pert_model.compute_gradients(X_test, mean=False)\n",
    "            p_logits = pert_model.compute_logits(X_test, mean=False)\n",
    "            for l, n in enumerate(n_samples):\n",
    "                pert_grads[i,l,j] += p_grads[:n].mean(axis=0)\n",
    "                pert_logits[i,l,j] += p_logits[:n].mean(axis=0)\n",
    "pert_grads /= ensemble_size\n",
    "pert_logits /= ensemble_size\n",
    "accs = (pert_logits.argmax(-1)==y_test).mean(axis=-1)\n",
    "accs_mean = (pert_logits.mean(axis=-3).argmax(-1)==y_test).mean(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topk, signs = get_top_k(5, pert_grads, return_sign=True)\n",
    "sa = np.zeros((len(sigmas), len(n_samples), n_inputs))\n",
    "for i in tqdm(range(len(sigmas))):\n",
    "    for j in range(len(n_samples)):\n",
    "        sa[i,j] = average_pairwise_score(topk[i,j], signs[i,j], top_k_sa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15, 5), dpi=100)\n",
    "metrics = [sa, accs*100]\n",
    "titles = ['Average Pairwise SA Similarity',\n",
    "          'Test Accuracy']\n",
    "ylabels = ['SA score', 'Accuracy (%)']\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(len(n_samples)):\n",
    "        q = np.quantile(metrics[i][:,j], [0.4, 0.5, 0.6], axis=-1)\n",
    "        ax[i].plot(sigmas, q[1], label=f'{n_samples[j]} perturbations')\n",
    "        ax[i].fill_between(sigmas, q[0], q[2], alpha=0.2)\n",
    "    ax[i].set_xlabel('$\\sigma$')\n",
    "    ax[i].set_ylabel(ylabels[i])\n",
    "    ax[i].set_title(titles[i])\n",
    "    ax[i].legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get names of all layers\n",
    "layer_names = [name for name, _ in model.named_parameters()]\n",
    "layer_titles = ['Layer ' + str(int(layer_names.split('.')[1])//2+1) + ' ' + layer_names.split('.')[-1].title() for layer_names in layer_names]\n",
    "layer_weights = [model.state_dict()[name] for name in layer_names]\n",
    "layer_norms = [lw.norm().item() for lw in layer_weights]\n",
    "layer_norms_cum = list(np.cumsum(layer_norms))\n",
    "print(layer_names)\n",
    "print(layer_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigs = np.linspace(0, 1000, 1000)\n",
    "nos = [torch.randn_like(layer_weights[0]) * sig for sig in sigs]\n",
    "no_norms = [no.norm().item() for no in nos]\n",
    "plt.plot(sigs, no_norms)\n",
    "n_connect = layer_weights[0].flatten().shape[0]\n",
    "plt.plot(sigs, sigs*np.sqrt(n_connect))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_sigmas = [0.2, 0.8,\n",
    "                  0.05, 1.0,\n",
    "                  0 to 0.5, 1.5,\n",
    "                  0 to 10, 0 to 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15, 5), dpi=100)\n",
    "\n",
    "ax[0].bar(np.arange(len(layer_names)), layer_norms)\n",
    "ax[0].set_xticks(np.arange(len(layer_names)), layer_titles, rotation=30)\n",
    "ax[0].set_ylabel('$\\ell_2$ Norm')\n",
    "\n",
    "ax[1].plot([0]+layer_norms_cum)\n",
    "ax[1].set_xticks(np.arange(len(layer_names)+1), ['']+layer_titles, rotation=30)\n",
    "ax[1].set_ylabel('Cumulative $\\ell_2$ Norm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 50\n",
    "sigma = 0.2\n",
    "n_trials = 10\n",
    "model = load_model(0)\n",
    "pert_grads = np.zeros((len(layer_names), n_trials, n_inputs, n_features))\n",
    "pert_logits = np.zeros((len(layer_names), n_trials, n_inputs, 2))\n",
    "for i in tqdm(range(len(layer_names))):\n",
    "    perturb_layers = layer_names[:i+1]\n",
    "    for j in range(n_trials):\n",
    "        pert_model = TabularModelPerturb(model, n_samples, sigma,\n",
    "                                         perturb_layers=perturb_layers)\n",
    "        pert_grads[i,j] = pert_model.compute_gradients(X_test, mean=True)\n",
    "        pert_logits[i,j] = pert_model.compute_logits(X_test, mean=True)\n",
    "accs = (pert_logits.argmax(axis=-1)==y_test).mean(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topk, signs = get_top_k(5, pert_grads, return_sign=True)\n",
    "sa = np.zeros((len(layer_names), n_inputs))\n",
    "for i in tqdm(range(len(layer_names))):\n",
    "    sa[i] = average_pairwise_score(topk[i], signs[i], top_k_sa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15, 5), dpi=100)\n",
    "metrics = [sa, accs*100]\n",
    "titles = ['Average Pairwise SA Similarity',\n",
    "          'Test Accuracy']\n",
    "ylabels = ['SA score', 'Accuracy (%)']\n",
    "\n",
    "for i in range(2):\n",
    "    q = np.quantile(metrics[i], [0.25, 0.5, 0.75], axis=-1)\n",
    "    ax[i].plot(range(len(layer_names)), q[1])\n",
    "    ax[i].fill_between(range(len(layer_names)), q[0], q[2], alpha=0.2)\n",
    "    ax[i].set_xlabel('layer')\n",
    "    ax[i].set_ylabel(ylabels[i])\n",
    "    ax[i].set_title(titles[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topk_gt, signs_gt = get_top_k(5, grads, return_sign=True)\n",
    "topk, signs = get_top_k(5, pert_grads, return_sign=True)\n",
    "gt_sa = np.zeros((len(layer_names), n_inputs))\n",
    "for i in tqdm(range(len(layer_names))):\n",
    "    gt_sa[i] = average_ground_truth_score(topk[i], signs[i], topk_gt, signs_gt, top_k_sa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n",
    "titles = ['Ground Truth SA', 'Accuracy']\n",
    "for i, metric in enumerate([gt_sa, accs]):\n",
    "    ax[i].boxplot([metric[i] for i in range(len(layer_names))], labels=layer_names)\n",
    "    #ax[i].set_xticks(rotation=30)\n",
    "    ax[i].set_ylabel(titles[i])\n",
    "    ax[i].set_title(f'{titles[i]} for each layer')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of perturbations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO:\n",
    "# - Optimize perturbation method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
