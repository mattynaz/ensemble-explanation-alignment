{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run once\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datasets\n",
    "from datasets.tabular import TabularModelPerturb\n",
    "import torch\n",
    "from datasets import get_model_class\n",
    "from util import get_statistics\n",
    "from similarity import get_top_k, average_pairwise_score, top_k_sa, average_ground_truth_score\n",
    "from style import bold\n",
    "from tqdm import tqdm\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'heloc'  # or 'german'\n",
    "n_models = 1000  # 1000 for german if needed\n",
    "random_sources = ['rs', 'loo']\n",
    "trainset, testset = datasets.load_dataset(name)\n",
    "\n",
    "# Random source\n",
    "random_source = random_sources[0]\n",
    "\n",
    "X_test, y_test = testset.data.numpy(), testset.labels.numpy()\n",
    "n_inputs, input_size = X_test.shape\n",
    "model_args = [input_size, datasets.tabular.layers[name]]\n",
    "optim = 'adam'\n",
    "epochs = 30\n",
    "epochs_mode = 40\n",
    "batch_size = 32\n",
    "lr = 0.0004\n",
    "# optim = 'sgd'\n",
    "# epochs = 20\n",
    "# epochs_mode = 30\n",
    "# lr = 0.1\n",
    "# batch_size = 64\n",
    "dropout = 0\n",
    "directory = f'models/{name}/{random_source}/{optim}_epochs{epochs}_lr{lr}_batch{batch_size}_dropout{dropout}'\n",
    "directory_mode = f'models/{name}/{random_source}/{optim}_epochs{epochs_mode}_lr{lr}_batch{batch_size}_dropout{dropout}'\n",
    "print(bold(\"Directory:\"), directory)\n",
    "print(bold(\"Directory mode:\"), directory_mode)\n",
    "\n",
    "def load_model(idx):\n",
    "    model_class = get_model_class(name)\n",
    "    model = model_class(*model_args)\n",
    "    state_dict = torch.load(f'{directory}/model_{idx}.pth')\n",
    "    model.load_state_dict(state_dict)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path to Glory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble sizes\n",
    "ensemble_sizes = [2, 4, 6, 8, 10, 20, 30]\n",
    "\n",
    "# Number of ensembles to sample for each ensemble size\n",
    "n_trials = 20\n",
    "\n",
    "# Top-k features to consider\n",
    "k = 5\n",
    "\n",
    "# Explanation type\n",
    "exp = 'smoothgrad'\n",
    "\n",
    "# Store no. inputs and no. features\n",
    "n_inputs, n_features = X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = ['perturb', 'average', 'majority', 'mode connect']\n",
    "topk = np.zeros((len(methods), len(ensemble_sizes), n_trials, n_inputs, k))\n",
    "signs = np.zeros((len(methods), len(ensemble_sizes), n_trials, n_inputs, k), dtype=int)\n",
    "test_accs = np.zeros((len(methods), len(ensemble_sizes), n_trials))\n",
    "\n",
    "for e, ensemble_size in enumerate(tqdm(ensemble_sizes)):\n",
    "    for i in range(n_trials):\n",
    "        # Sample models\n",
    "        model_idx = np.random.choice(n_models, ensemble_size, replace=False)\n",
    "        for j, method in enumerate(methods):\n",
    "            dir = directory_mode if method == 'mode connect' else directory\n",
    "            grads, preds = get_statistics(model_idx, method, dir, exp=exp)\n",
    "            topk[j, e, i], signs[j, e, i] = get_top_k(k=k, X=grads*X_test, return_sign=True)\n",
    "            test_accs[j, e, i] = (preds == y_test).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities = np.zeros((len(methods), len(ensemble_sizes), n_inputs))\n",
    "for e, ensemble_size in enumerate(tqdm(ensemble_sizes)):\n",
    "    for i, method in enumerate(methods):\n",
    "        similarities[i, e] = average_pairwise_score(topk[i, e], signs[i, e], top_k_sa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15, 5), dpi=150)\n",
    "plt.suptitle('Comparison of different ensemble methods', fontweight='bold', y=1.01, fontsize=15)\n",
    "titles = ['Average pairwise similarity', 'Test accuracy']\n",
    "ylabs = ['Average pairwise similarity', 'Accuracy (%)']\n",
    "\n",
    "for i, method in enumerate(methods):\n",
    "    q = np.quantile(similarities[i], [0.4, 0.5, 0.6], axis=1)\n",
    "    ax[0].plot(ensemble_sizes, q[1], label=method)\n",
    "    ax[0].fill_between(ensemble_sizes, q[0], q[2], alpha=0.2)\n",
    "    q = np.quantile(100*test_accs[i], [0.25, 0.75], axis=1)\n",
    "    ax[1].plot(ensemble_sizes, q[0], label=method)\n",
    "    ax[1].fill_between(ensemble_sizes, q[0], q[1], alpha=0.2)\n",
    "\n",
    "for i in range(2):\n",
    "    ax[i].set_xlabel('Ensemble size')\n",
    "    ax[i].set_xticks(ensemble_sizes)\n",
    "    ax[i].legend(loc='lower right')\n",
    "    ax[i].set_ylabel(ylabs[i])\n",
    "    ax[i].set_title(titles[i])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15, 5), dpi=150)\n",
    "plt.suptitle('Comparison of different ensemble methods', fontweight='bold', y=1.01, fontsize=15)\n",
    "titles = ['Average pairwise similarity', 'Test accuracy']\n",
    "ylabs = ['Average pairwise similarity', 'Accuracy (%)']\n",
    "\n",
    "for i, method in enumerate(methods):\n",
    "    q = np.quantile(similarities[i], [0.4, 0.5, 0.6], axis=1)\n",
    "    ax[0].plot(ensemble_sizes, q[1], label=method)\n",
    "    ax[0].fill_between(ensemble_sizes, q[0], q[2], alpha=0.2)\n",
    "    q = np.quantile(100*test_accs[i], [0.25, 0.75], axis=1)\n",
    "    ax[1].plot(ensemble_sizes, q[0], label=method)\n",
    "    ax[1].fill_between(ensemble_sizes, q[0], q[1], alpha=0.2)\n",
    "\n",
    "for i in range(2):\n",
    "    ax[i].set_xlabel('Ensemble size')\n",
    "    ax[i].set_xticks(ensemble_sizes)\n",
    "    ax[i].legend(loc='lower right')\n",
    "    ax[i].set_ylabel(ylabs[i])\n",
    "    ax[i].set_title(titles[i])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15, 5), dpi=150)\n",
    "plt.suptitle('Comparison of different ensemble methods', fontweight='bold', y=1.01, fontsize=15)\n",
    "titles = ['Average pairwise similarity', 'Test accuracy']\n",
    "ylabs = ['Average pairwise similarity', 'Accuracy (%)']\n",
    "\n",
    "for i, method in enumerate(methods):\n",
    "    q = np.quantile(similarities[i], [0.4, 0.5, 0.6], axis=1)\n",
    "    ax[0].plot(ensemble_sizes, q[1], label=method)\n",
    "    ax[0].fill_between(ensemble_sizes, q[0], q[2], alpha=0.2)\n",
    "    q = np.quantile(100*test_accs[i], [0.25, 0.75], axis=1)\n",
    "    ax[1].plot(ensemble_sizes, q[0], label=method)\n",
    "    ax[1].fill_between(ensemble_sizes, q[0], q[1], alpha=0.2)\n",
    "\n",
    "for i in range(2):\n",
    "    ax[i].set_xlabel('Ensemble size')\n",
    "    ax[i].set_xticks(ensemble_sizes)\n",
    "    ax[i].legend(loc='lower right')\n",
    "    ax[i].set_ylabel(ylabs[i])\n",
    "    ax[i].set_title(titles[i])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15, 5), dpi=150)\n",
    "plt.suptitle('Comparison of different ensemble methods', fontweight='bold', y=1.01, fontsize=15)\n",
    "titles = ['Average pairwise similarity', 'Test accuracy']\n",
    "ylabs = ['Average pairwise similarity', 'Accuracy (%)']\n",
    "\n",
    "for i, method in enumerate(methods):\n",
    "    q = np.quantile(similarities[i], [0.4, 0.5, 0.6], axis=1)\n",
    "    ax[0].plot(ensemble_sizes, q[1], label=method)\n",
    "    ax[0].fill_between(ensemble_sizes, q[0], q[2], alpha=0.2)\n",
    "    q = np.quantile(100*test_accs[i], [0.25, 0.75], axis=1)\n",
    "    ax[1].plot(ensemble_sizes, q[0], label=method)\n",
    "    ax[1].fill_between(ensemble_sizes, q[0], q[1], alpha=0.2)\n",
    "\n",
    "for i in range(2):\n",
    "    ax[i].set_xlabel('Ensemble size')\n",
    "    ax[i].set_xticks(ensemble_sizes)\n",
    "    ax[i].legend(loc='lower right')\n",
    "    ax[i].set_ylabel(ylabs[i])\n",
    "    ax[i].set_title(titles[i])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15, 5), dpi=150)\n",
    "plt.suptitle('Comparison of different ensemble methods', fontweight='bold', y=1.01, fontsize=15)\n",
    "titles = ['Average pairwise similarity', 'Test accuracy']\n",
    "ylabs = ['Average pairwise similarity', 'Accuracy (%)']\n",
    "\n",
    "for i, method in enumerate(methods):\n",
    "    q = np.quantile(similarities[i], [0.4, 0.5, 0.6], axis=1)\n",
    "    ax[0].plot(ensemble_sizes, q[1], label=method)\n",
    "    ax[0].fill_between(ensemble_sizes, q[0], q[2], alpha=0.2)\n",
    "    q = np.quantile(100*test_accs[i], [0.25, 0.75], axis=1)\n",
    "    ax[1].plot(ensemble_sizes, q[0], label=method)\n",
    "    ax[1].fill_between(ensemble_sizes, q[0], q[1], alpha=0.2)\n",
    "\n",
    "for i in range(2):\n",
    "    ax[i].set_xlabel('Ensemble size')\n",
    "    ax[i].set_xticks(ensemble_sizes)\n",
    "    ax[i].legend(loc='lower right')\n",
    "    ax[i].set_ylabel(ylabs[i])\n",
    "    ax[i].set_title(titles[i])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15, 5), dpi=150)\n",
    "plt.suptitle('Comparison of different ensemble methods', fontweight='bold', y=1.01, fontsize=15)\n",
    "titles = ['Average pairwise similarity', 'Test accuracy']\n",
    "ylabs = ['Average pairwise similarity', 'Accuracy (%)']\n",
    "\n",
    "for i, method in enumerate(methods):\n",
    "    q = np.quantile(similarities[i], [0.4, 0.5, 0.6], axis=1)\n",
    "    ax[0].plot(ensemble_sizes, q[1], label=method)\n",
    "    ax[0].fill_between(ensemble_sizes, q[0], q[2], alpha=0.2)\n",
    "    q = np.quantile(100*test_accs[i], [0.25, 0.75], axis=1)\n",
    "    ax[1].plot(ensemble_sizes, q[0], label=method)\n",
    "    ax[1].fill_between(ensemble_sizes, q[0], q[1], alpha=0.2)\n",
    "\n",
    "for i in range(2):\n",
    "    ax[i].set_xlabel('Ensemble size')\n",
    "    ax[i].set_xticks(ensemble_sizes)\n",
    "    ax[i].legend(loc='lower right')\n",
    "    ax[i].set_ylabel(ylabs[i])\n",
    "    ax[i].set_title(titles[i])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15, 5), dpi=150)\n",
    "plt.suptitle('Comparison of different ensemble methods', fontweight='bold', y=1.01, fontsize=15)\n",
    "titles = ['Average pairwise similarity', 'Test accuracy']\n",
    "ylabs = ['Average pairwise similarity', 'Accuracy (%)']\n",
    "\n",
    "for i, method in enumerate(methods):\n",
    "    q = np.quantile(similarities[i], [0.4, 0.5, 0.6], axis=1)\n",
    "    ax[0].plot(ensemble_sizes, q[1], label=method)\n",
    "    ax[0].fill_between(ensemble_sizes, q[0], q[2], alpha=0.2)\n",
    "    q = np.quantile(100*test_accs[i], [0.25, 0.75], axis=1)\n",
    "    ax[1].plot(ensemble_sizes, q[0], label=method)\n",
    "    ax[1].fill_between(ensemble_sizes, q[0], q[1], alpha=0.2)\n",
    "\n",
    "for i in range(2):\n",
    "    ax[i].set_xlabel('Ensemble size')\n",
    "    ax[i].set_xticks(ensemble_sizes)\n",
    "    ax[i].legend(loc='lower right')\n",
    "    ax[i].set_ylabel(ylabs[i])\n",
    "    ax[i].set_title(titles[i])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15, 5), dpi=150)\n",
    "plt.suptitle('Comparison of different ensemble methods', fontweight='bold', y=1.01, fontsize=15)\n",
    "titles = ['Average pairwise similarity', 'Test accuracy']\n",
    "ylabs = ['Average pairwise similarity', 'Accuracy (%)']\n",
    "\n",
    "for i, method in enumerate(methods):\n",
    "    q = np.quantile(similarities[i], [0.4, 0.5, 0.6], axis=1)\n",
    "    ax[0].plot(ensemble_sizes, q[1], label=method)\n",
    "    ax[0].fill_between(ensemble_sizes, q[0], q[2], alpha=0.2)\n",
    "    q = np.quantile(100*test_accs[i], [0.25, 0.75], axis=1)\n",
    "    ax[1].plot(ensemble_sizes, q[0], label=method)\n",
    "    ax[1].fill_between(ensemble_sizes, q[0], q[1], alpha=0.2)\n",
    "\n",
    "for i in range(2):\n",
    "    ax[i].set_xlabel('Ensemble size')\n",
    "    ax[i].set_xticks(ensemble_sizes)\n",
    "    ax[i].legend(loc='lower right')\n",
    "    ax[i].set_ylabel(ylabs[i])\n",
    "    ax[i].set_title(titles[i])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads = np.array([np.load(f'{directory}/grads_{idx}.npy') for idx in range(n_models)])\n",
    "gt, signs_gt = get_top_k(k, grads.mean(axis=0), return_sign=True)\n",
    "\n",
    "grads_perturb = np.array([np.load(f'{directory}/grads_perturb_{idx}.npy') for idx in range(n_models)])\n",
    "gt_perturb, signs_gt_perturb = get_top_k(k, grads_perturb.mean(axis=0), return_sign=True)\n",
    "\n",
    "similarities_gt = np.zeros((len(methods), len(ensemble_sizes), n_inputs))\n",
    "similarities_gt_perturb = np.zeros((len(methods), len(ensemble_sizes), n_inputs))\n",
    "for e, ensemble_size in enumerate(tqdm(ensemble_sizes)):\n",
    "    for i, method in enumerate(methods):\n",
    "        similarities_gt[i, e] = average_ground_truth_score(topk[i, e], signs[i, e], gt, signs_gt, top_k_sa)\n",
    "        similarities_gt_perturb[i, e] = average_ground_truth_score(topk[i, e], signs[i, e], gt_perturb, signs_gt_perturb, top_k_sa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(13, 5), dpi=150)\n",
    "labels = ['Perturb Models + Average', 'Average']\n",
    "metrics = [similarities_gt, similarities_gt_perturb]\n",
    "titles = ['Similarity to gradient ground truth', 'Similarity to perturbed gradient ground truth']\n",
    "plt.suptitle('Effect of ensemble size on average similarity to ground truth for various ensemble methods',\n",
    "             fontweight='bold', y=1.01, fontsize=15)\n",
    "for i in range(2):\n",
    "    for j, metric in enumerate(metrics[i]):\n",
    "        q = np.quantile(metric, [0.4, 0.5, 0.6], axis=1)\n",
    "        ax[i].plot(ensemble_sizes, q[1], label=methods[j])\n",
    "        ax[i].fill_between(ensemble_sizes, q[0], q[2], alpha=0.2)\n",
    "    ax[i].set_xlabel('Ensemble size')\n",
    "    ax[i].set_title(titles[i])\n",
    "    ax[i].set_ylim(0.7, 1)\n",
    "    ax[i].set_xticks(ensemble_sizes)\n",
    "    ax[i].set_ylabel('Signed-Agreement (SA)')\n",
    "    ax[i].legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(13, 5), dpi=150)\n",
    "labels = ['Perturb Models + Average', 'Average']\n",
    "metrics = [similarities_gt, similarities_gt_perturb]\n",
    "titles = ['Similarity to gradient ground truth', 'Similarity to perturbed gradient ground truth']\n",
    "plt.suptitle('Effect of ensemble size on average similarity to ground truth for various ensemble methods',\n",
    "             fontweight='bold', y=1.01, fontsize=15)\n",
    "for i in range(2):\n",
    "    for j, metric in enumerate(metrics[i]):\n",
    "        q = np.quantile(metric, [0.4, 0.5, 0.6], axis=1)\n",
    "        ax[i].plot(ensemble_sizes, q[1], label=methods[j])\n",
    "        ax[i].fill_between(ensemble_sizes, q[0], q[2], alpha=0.2)\n",
    "    ax[i].set_xlabel('Ensemble size')\n",
    "    ax[i].set_title(titles[i])\n",
    "    ax[i].set_ylim(0.7, 1)\n",
    "    ax[i].set_xticks(ensemble_sizes)\n",
    "    ax[i].set_ylabel('Signed-Agreement (SA)')\n",
    "    ax[i].legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(13, 5), dpi=150)\n",
    "labels = ['Perturb Models + Average', 'Average']\n",
    "metrics = [similarities_gt, similarities_gt_perturb]\n",
    "titles = ['Similarity to gradient ground truth', 'Similarity to perturbed gradient ground truth']\n",
    "plt.suptitle('Effect of ensemble size on average similarity to ground truth for various ensemble methods',\n",
    "             fontweight='bold', y=1.01, fontsize=15)\n",
    "for i in range(2):\n",
    "    for j, metric in enumerate(metrics[i]):\n",
    "        q = np.quantile(metric, [0.4, 0.5, 0.6], axis=1)\n",
    "        ax[i].plot(ensemble_sizes, q[1], label=methods[j])\n",
    "        ax[i].fill_between(ensemble_sizes, q[0], q[2], alpha=0.2)\n",
    "    ax[i].set_xlabel('Ensemble size')\n",
    "    ax[i].set_title(titles[i])\n",
    "    ax[i].set_ylim(0.7, 1)\n",
    "    ax[i].set_xticks(ensemble_sizes)\n",
    "    ax[i].set_ylabel('Signed-Agreement (SA)')\n",
    "    ax[i].legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Metrics (need to adjust similarity_metrics.py to suit our uses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarity between ensembles\n",
    "SA_avg = np.zeros((len(ensemble_sizes), n_inputs))\n",
    "SA_perturb = np.zeros((len(ensemble_sizes), n_inputs))\n",
    "\n",
    "# Similarity to gradient ground truth\n",
    "SA_avg_gt = np.zeros((len(ensemble_sizes), n_inputs))\n",
    "SA_perturb_gt = np.zeros((len(ensemble_sizes), n_inputs))\n",
    "\n",
    "# Similarity to perturbed gradient ground truth\n",
    "SA_avg_gt_perturb = np.zeros((len(ensemble_sizes), n_inputs))\n",
    "SA_perturb_gt_perturb = np.zeros((len(ensemble_sizes), n_inputs))\n",
    "\n",
    "grads = np.array([np.load(f'{directory}/grads_{idx}.npy') for idx in range(n_models)])\n",
    "gt, signs_gt = get_top_k(k, grads.mean(axis=0), return_sign=True)\n",
    "\n",
    "grads_perturb = np.array([np.load(f'{directory}/grads_perturb_{idx}.npy') for idx in range(n_models)])\n",
    "gt_perturb, signs_gt_perturb = get_top_k(k, grads_perturb.mean(axis=0), return_sign=True)\n",
    "\n",
    "for i in tqdm(range(len(ensemble_sizes))):\n",
    "    SA_avg[i] = average_pairwise_score(topk_avg[i], signs_avg[i], top_k_sa)\n",
    "    SA_perturb[i] = average_pairwise_score(topk_perturb[i], signs_perturb[i], top_k_sa)\n",
    "    \n",
    "    SA_avg_gt[i] = average_ground_truth_score(topk_avg[i], signs_avg[i], gt, signs_gt, top_k_sa)\n",
    "    SA_perturb_gt[i] = average_ground_truth_score(topk_perturb[i], signs_perturb[i], gt, signs_gt, top_k_sa)\n",
    "\n",
    "    SA_avg_gt_perturb[i] = average_ground_truth_score(topk_avg[i], signs_avg[i], gt_perturb, signs_gt_perturb, top_k_sa)\n",
    "    SA_perturb_gt_perturb[i] = average_ground_truth_score(topk_perturb[i], signs_perturb[i], gt_perturb, signs_gt_perturb, top_k_sa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(15, 5), dpi=150)\n",
    "labels = ['Perturb Models + Average', 'Average']\n",
    "metrics = [[SA_perturb, SA_avg],\n",
    "           [SA_perturb_gt, SA_avg_gt],\n",
    "           [SA_perturb_gt_perturb, SA_avg_gt_perturb]]\n",
    "titles = ['Average pairwise similarity', 'Similarity to gradient ground truth', 'Similarity to perturbed gradient ground truth']\n",
    "plt.suptitle('Effect of ensemble size on average pairwise similarity for various ensemble methods', fontweight='bold', y=1.01, fontsize=15)\n",
    "for i in range(3):\n",
    "    for j, metric in enumerate(metrics[i]):\n",
    "        q = np.quantile(metric, [0.4, 0.5, 0.6], axis=1)\n",
    "        ax[i].plot(ensemble_sizes, q[1], label=labels[j])\n",
    "        ax[i].fill_between(ensemble_sizes, q[0], q[2], alpha=0.2)\n",
    "    ax[i].set_xlabel('Ensemble size')\n",
    "    ax[i].set_title(titles[i])\n",
    "    ax[i].set_ylim(0.7, 1)\n",
    "    ax[i].set_xticks(ensemble_sizes)\n",
    "    ax[i].set_ylabel('Signed-Agreement (SA)')\n",
    "    ax[i].legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Misc Plots"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to load/visualize gradients"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay so some gradients are zero... moreso if you do softmax gradient instead of logit gradient\n",
    "\n",
    "Might just be a floating point error (could try float64 instead of float32 but will double the storage size)\n",
    "\n",
    "softmax gradient and logit gradient should be the same (?), just softmax is magnitudes smaller"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top k and comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_sizes = [5, 10, 20, 40, 70, 100]\n",
    "# Randomly sample size_ensemble indices from 0 to n_models for each ensemble size, load grads, and get top k grads\n",
    "top5 = []\n",
    "n_test = 5\n",
    "counts = np.zeros((n_test, len(ensemble_sizes), n_features))\n",
    "for i in range(n_test):\n",
    "    fig, axs = plt.subplots(1, len(ensemble_sizes), figsize=(len(ensemble_sizes)*4, 4), dpi=100)\n",
    "    for j, size_ensemble in enumerate(ensemble_sizes):\n",
    "        indices = np.random.choice(n_models, size_ensemble, replace=False)  # should do this many times\n",
    "        grads = np.array([np.load(f'{directory}/{random_source}_grads_{idx}.npy') for idx in indices])\n",
    "        top5.append(np.array([get_top_k(grads[idx], k=5) for idx in range(size_ensemble)]))\n",
    "        un, co = np.unique(top5[j][:, i].flatten(), return_counts=True)\n",
    "        axs[j].bar(un, co)\n",
    "        axs[j].set_xlabel('Feature Index')\n",
    "        axs[j].set_ylabel('Frequency')\n",
    "        axs[j].set_title(f'Ensemble Size: {size_ensemble}')\n",
    "        counts[i, j, un] = co\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, n_test, figsize=(n_test*5, 4), dpi=100)\n",
    "plt.subplots_adjust(wspace=0.3)\n",
    "for i in range(n_test):\n",
    "    counts_norm = counts[i]/np.sum(counts[i], axis=1)[:, None]\n",
    "    axs[i].plot(ensemble_sizes, np.linalg.norm(counts_norm-counts_norm[-1], axis=1, ord=1))\n",
    "    axs[i].set_xlabel('Ensemble size')\n",
    "    axs[i].set_ylabel('$\\ell_1$ distance from last ensemble size')\n",
    "    axs[i].set_title(f'Input {i}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute entropy of each feature distribution in counts\n",
    "from scipy.stats import entropy\n",
    "# Plot entropy of counts for each input\n",
    "fig, axs = plt.subplots(1, n_test, figsize=(n_test*4, 4), dpi=100)\n",
    "for i in range(n_test):\n",
    "    full_counts_norm = counts[i, -1]/np.sum(counts[i, -1])\n",
    "    counts[0]/np.sum(counts[0], axis=1)[:, None]\n",
    "    axs[i].plot(ensemble_sizes, entropy(counts[i], axis=1))\n",
    "    axs[i].set_xlabel('Ensemble size')\n",
    "    axs[i].set_ylabel('Entropy')\n",
    "    axs[i].set_title(f'Input {i}')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
